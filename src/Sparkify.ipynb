{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML-Customer Churn Prediction with sparkify dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20190321155918-0000\n",
      "KERNEL_ID = 55dd955c-2ce1-4b4c-8376-ca7486cb47e5\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Imputer, CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If running on IBM watson studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='293'),\n",
       " Row(artist=\"John Brown's Body\", auth='Logged In', firstName='Sawyer', gender='M', itemInSession=74, lastName='Larson', length=380.21179, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='PUT', page='NextSong', registration=1538069638000, sessionId=97, song='Bulls', status=200, ts=1538352025000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='98')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-dcd80924-823c-4e98-a4a1-9008ddc1a9a8',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token',\n",
    "    'api_key': 'kV730WOitMi206CFroom3MFI13vBRa2kD8MNECWTx1tQ'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_522f6938d423453eb1317cb06aa5d727_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "df = spark.read.json(cos.url('medium-sparkify-dataset.json', 'sparkify-donotdelete-pr-1hvb6rdsl1zaqx'))\n",
    "df.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If Running on local machine:\n",
    "\n",
    "uncomment the following code block and input the data path to execute the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PATH = #path to json data\n",
    "# spark = SparkSession.builder \\\n",
    "#     .master(\"local\") \\\n",
    "#     .appName(\"Sparkify project\") \\\n",
    "#     .getOrCreate()\n",
    "# # Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# # Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# # PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "# df = spark.read.json(PATH)\n",
    "# df.persist()\n",
    "# df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rows with len(userid) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\"length(userid) != 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|          location|method|    page| registration|sessionId|         song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "|Martin Orford|Logged In|   Joseph|     M|           20| Morales|597.55057| free|Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid = df.dropna(how = 'any', subset = ['userId', 'sessionId'])\n",
    "df_valid.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill empty entries in  columns:\n",
    "StringIndexer in pyspark cannot deal with empty entries. The empty entries in columns with string type should be filled with \"None\" to avoid exception. The numeric columns are filled with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = ['artist', 'gender', 'location', 'song', 'userAgent']\n",
    "num_columns = ['itemInSession', 'length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_string_null(df, columns):\n",
    "    '''\n",
    "        input: \n",
    "                df : spark data frame.\n",
    "                columns : columns in df that are of string type.\n",
    "        \n",
    "        return: transformed data frame.\n",
    "    '''\n",
    "    for column in columns:\n",
    "        df = df.withColumn(column, F.when(F.isnull(F.col(column)),\"NONE\").otherwise(F.col(column)))\n",
    "    return df\n",
    "\n",
    "def fill_num_null(df, columns):\n",
    "    #transform to double type\n",
    "    for column in columns:\n",
    "        df = df.withColumn(column, df[column].cast(DoubleType()))\n",
    "        \n",
    "    imputer = Imputer(inputCols = columns, outputCols = columns)\n",
    "    df = imputer.fit(df).transform(df)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = fill_string_null(df_valid, string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = fill_num_null(df_valid, num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Create a column `Churn` to use as the label for your model. `Cancellation Confirmation`,`Cancel`,`Submit Downgrade` and `Downgrade` events are considered to define churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explory visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+------+-------------+--------+------------------+-----+-------------+------+---------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|artist|     auth|firstName|gender|itemInSession|lastName|            length|level|     location|method|     page| registration|sessionId|song|status|           ts|           userAgent|userId|\n",
      "+------+---------+---------+------+-------------+--------+------------------+-----+-------------+------+---------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|  NONE|Logged In|    Sofia|     F|        268.0|  Gordon|248.66459278007738| paid|Rochester, MN|   GET|Downgrade|1533175710000|      162|NONE|   200|1538352336000|\"Mozilla/5.0 (Mac...|   163|\n",
      "|  NONE|Logged In|    Sofia|     F|        296.0|  Gordon|248.66459278007738| paid|Rochester, MN|   GET|Downgrade|1533175710000|      162|NONE|   200|1538358258000|\"Mozilla/5.0 (Mac...|   163|\n",
      "+------+---------+---------+------+-------------+--------+------------------+-----+-------------+------+---------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.filter('page == \"Cancellation Confirmation\" or page == \"Downgrade\" or page == \"Cancel\" or page == \"Submit Downgrade\"').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.select('page').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Churn for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_class = udf(lambda x : 1 if x in [\"Cancellation Confirmation\", \"Downgrade\", 'Submit Downgrade', 'Cancel'] else 0, IntegerType())\n",
    "df_valid = df_valid.withColumn('Churn', label_class(df_valid.page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Churn| count|\n",
      "+-----+------+\n",
      "|    1|  4126|\n",
      "|    0|523879|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.groupby(df_valid.Churn).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the following fields as features to train our model:\n",
    "\n",
    "* string_columns = ['artist', 'gender', 'location', 'song', 'userAgent']\n",
    "\n",
    "* num_columns = ['itemInSession', 'length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode gender feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeFeatures(df, columns):\n",
    "    '''\n",
    "        encode categorical features to numerical features.\n",
    "        input:\n",
    "            df: spark dataframe.\n",
    "            columns: categorical feature columns.\n",
    "    '''\n",
    "    for column in columns:\n",
    "        indexer = StringIndexer(inputCol = column, outputCol = column + \"_encode\")\n",
    "        df = indexer.fit(df).transform(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|          location|method|    page| registration|sessionId|         song|status|           ts|           userAgent|userId|Churn|artist_encode|gender_encode|location_encode|song_encode|userAgent_encode|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+\n",
      "|Martin Orford|Logged In|   Joseph|     M|         20.0| Morales|597.55057| free|Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|    0|       3931.0|          0.0|           26.0|     4600.0|             0.0|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid = encodeFeatures(df_valid, string_columns)\n",
    "df_valid.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate feature column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* string_columns = ['artist', 'gender', 'location', 'song', 'userAgent']\n",
    "* num_columns = ['itemInSession', 'length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_valid.drop('features')\n",
    "df_valid = df_valid.drop('rawFeatures')\n",
    "assembler = VectorAssembler(inputCols = [\"artist_encode\",\"gender_encode\",\\\n",
    "                                       \"location_encode\",\"song_encode\",\\\n",
    "                                       \"userAgent_encode\", \"itemInSession\",\\\n",
    "                                       \"length\"], outputCol = \"rawFeatures\")\n",
    "df_valid = assembler.transform(df_valid)\n",
    "# normalize features\n",
    "scaler = Normalizer(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "df_valid = scaler.transform(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+--------------------+--------------------+\n",
      "|           artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|              song|status|           ts|           userAgent|userId|Churn|artist_encode|gender_encode|location_encode|song_encode|userAgent_encode|         rawFeatures|            features|\n",
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+--------------------+--------------------+\n",
      "|    Martin Orford|Logged In|   Joseph|     M|         20.0| Morales|597.55057| free|  Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|     Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|    0|       3931.0|          0.0|           26.0|     4600.0|             0.0|[3931.0,0.0,26.0,...|[0.64650667128954...|\n",
      "|John Brown's Body|Logged In|   Sawyer|     M|         74.0|  Larson|380.21179| free|Houston-The Woodl...|   PUT|NextSong|1538069638000|       97|             Bulls|   200|1538352025000|\"Mozilla/5.0 (Mac...|    98|    0|       2577.0|          0.0|            9.0|    12367.0|             5.0|[2577.0,0.0,9.0,1...|[0.20389945259065...|\n",
      "|          Afroman|Logged In| Maverick|     M|        184.0|Santiago|202.37016| paid|Orlando-Kissimmee...|   PUT|NextSong|1535953455000|      178|Because I Got High|   200|1538352118000|\"Mozilla/5.0 (Mac...|   179|    0|       1320.0|          0.0|           27.0|     3436.0|             5.0|[1320.0,0.0,27.0,...|[0.35761906426868...|\n",
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test sets. Evaluate the metrics of the various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only paid users are of interest.\n",
    "rest, validation = df_valid.filter(df_valid.level == 'paid').randomSplit([0.75, 0.25], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weight column to solve imbalanced dataset problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use the information of training dataset to calculate the balanced class weight. \n",
    "# Do not use information from test dataset to avoid fake score.\n",
    "def addWeight(df_train, df_test, gammas = [1]):\n",
    "    '''\n",
    "        add weight column to solve imbalanced problem.\n",
    "        The weightCol field is used in logistic regression model.\n",
    "        input:\n",
    "            df_train : training data frame.\n",
    "            df_test : test data frame.\n",
    "            \n",
    "        return:\n",
    "            training and test dataframe after transformation. \n",
    "    '''\n",
    "    total_instances = df_train.count()\n",
    "    negative_instances = df_train.filter(df_train.Churn == 0).count()\n",
    "    for gamma in gammas:\n",
    "        balance_ratio = (total_instances - negative_instances * gamma) / total_instances\n",
    "        print(\"gamma=%.3f, balance_ratio = %.3f\"%(gamma, balance_ratio))\n",
    "        add_weightCol = udf(lambda x : balance_ratio if x == 0 else (1 - balance_ratio), DoubleType())\n",
    "        df_train = df_train.withColumn('weightCol%d'%(gamma*1000), add_weightCol(df_train.Churn))\n",
    "        df_test = df_test.withColumn('weightCol%d'%(gamma*1000), add_weightCol(df_test.Churn))\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma=0.800, balance_ratio = 0.208\n",
      "gamma=0.900, balance_ratio = 0.109\n",
      "gamma=0.950, balance_ratio = 0.059\n",
      "gamma=0.990, balance_ratio = 0.020\n",
      "gamma=1.000, balance_ratio = 0.010\n",
      "gamma=1.001, balance_ratio = 0.009\n"
     ]
    }
   ],
   "source": [
    "rest, validation = addWeight(rest, validation, gammas = [0.8, 0.9, 0.95, 0.99, 1, 1.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Churn|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.33881679736696...|\n",
      "|       0.0|    0|[0.33896164916210...|\n",
      "|       0.0|    0|[0.16668946441272...|\n",
      "|       0.0|    0|[0.55517912002349...|\n",
      "|       0.0|    0|[0.49187204269535...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a LogisticRegression model.\n",
    "lgr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                         weightCol = 'weightCol800', maxIter = 50, regParam = 0)\n",
    "\n",
    "#pipeline = Pipeline(stages=[gbt])\n",
    "pipeline = Pipeline(stages=[lgr])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(rest)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(validation)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Churn\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Evalulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    #add one smoothness\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    #add one smoothness\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1_score(tp, tn, fp, fn):\n",
    "    pre = precision(tp, fp)\n",
    "    rec = recall(tp, fn)\n",
    "    return 2 * pre * rec / (pre + rec)\n",
    "\n",
    "def accuracy(tp, tn, fp, fn):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(predictions):\n",
    "    tp = predictions.filter(predictions.prediction == predictions.Churn).\\\n",
    "                 filter(predictions.prediction == 1).count()\n",
    "    tn = predictions.filter(predictions.prediction == predictions.Churn).\\\n",
    "                     filter(predictions.prediction == 0).count()\n",
    "    fp = predictions.filter(predictions.prediction != predictions.Churn).\\\n",
    "                     filter(predictions.prediction == 1).count()\n",
    "    fn = predictions.filter(predictions.prediction != predictions.Churn).\\\n",
    "                     filter(predictions.prediction == 0).count()\n",
    "    print(\"true positive = \", tp)\n",
    "    print(\"true negative = \", tn)\n",
    "    print(\"false positive = \", fp)\n",
    "    print(\"false negative = \", fn)\n",
    "    print(\"precision = \",precision(tp, fp))\n",
    "    print(\"recall = \",recall(tp, fn))\n",
    "    print(\"f1_score = \",f1_score(tp, tn, fp, fn))\n",
    "    print(\"accuracy = \", accuracy(tp, tn, fp, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive =  0\n",
      "true negative =  103407\n",
      "false positive =  0\n",
      "false negative =  1008\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-a535027cfbf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-5f91d33977b9>\u001b[0m in \u001b[0;36mprint_metrics\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"false positive = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"false negative = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1_score = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-997e5661817b>\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(tp, fp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#add one smoothness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Running the above code block throws division by zero error to us. The reason is that the class is highly imbalanced. It will easily get very high metrics by predicting all samples to be negative. To resolve this problem, we should select 'weightCol' with higher **gamma** value in fitting the model. e.g. 'weightCol990', 'weightCol1000'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a LogisticRegression model.\n",
    "lgr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                         weightCol = 'weightCol1000', maxIter = 50, regParam = 0)\n",
    "\n",
    "#pipeline = Pipeline(stages=[gbt])\n",
    "pipeline = Pipeline(stages=[lgr])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(rest)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive =  559\n",
      "true negative =  95173\n",
      "false positive =  8234\n",
      "false negative =  449\n",
      "precision =  0.06357329694074833\n",
      "recall =  0.5545634920634921\n",
      "f1_score =  0.11406999285787167\n",
      "accuracy =  0.9168414499832399\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear support vector machine:\n",
    "\n",
    "support vector machine is time-consuming to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                        weightCol = 'weightCol1000', maxIter = 50, regParam = 0)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "#pipeline = Pipeline(stages=[gbt])\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "\n",
    "# Train model. This also runs the indexers.\n",
    "model = pipeline.fit(rest)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_svm = model.transform(validation)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions_svm.select(\"prediction\", \"Churn\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate four important metrics\n",
    "tp = predictions_svm.filter(predictions_svm.prediction == predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 1).count()\n",
    "tn = predictions_svm.filter(predictions_svm.prediction == predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 0).count()\n",
    "fp = predictions_svm.filter(predictions_svm.prediction != predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 1).count()\n",
    "fn = predictions_svm.filter(predictions_svm.prediction != predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Samples :  1008\n",
      "True Negative Samples :  87468\n",
      "False Positive Samples :  15939\n",
      "False Negative Samples :  0\n"
     ]
    }
   ],
   "source": [
    "#Print for important metrics\n",
    "print(\"True Positive Samples : \", tp)\n",
    "print(\"True Negative Samples : \", tn)\n",
    "print(\"False Positive Samples : \", fp)\n",
    "print(\"False Negative Samples : \", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(predictions_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning models using k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                        threshold = 0.6, maxIter = 5, regParam = 0)\n",
    "\n",
    "pipeline = Pipeline(stages=[lgr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use k-fold cross validation to select best\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lgr.weightCol, ['weightCol800','weightCol900','weightCol950',\\\n",
    "                             'weightCol990','weightCol1000'])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(rawPredictionCol = 'prediction', \\\n",
    "                                                                  labelCol = 'Churn', metricName = 'areaUnderPR'),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit cross validation model on training dataset\n",
    "cvModel_q1 = crossval.fit(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction on validation dataset\n",
    "predictions = cvModel_q1.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009737378859114671,\n",
       " 0.009737378859114671,\n",
       " 0.009737378859114671,\n",
       " 0.05115955214467296,\n",
       " 0.04692467914905578]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# select the hyperparameter with highest metric\n",
    "cvModel_q1.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation conclusin:\n",
    "\n",
    "This suggests that gamma = 0.99 gives us best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use best hyper-parameter to train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Churn|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[2916.0,0.0,7.0,8...|\n",
      "|       0.0|    0|[2916.0,0.0,57.0,...|\n",
      "|       0.0|    0|[2916.0,0.0,25.0,...|\n",
      "|       0.0|    0|[6571.0,1.0,48.0,...|\n",
      "|       0.0|    0|[7109.0,1.0,101.0...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a LogisticRegression model.\n",
    "lgr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                        weightCol = 'weightCol990', threshold = 0.6, maxIter = 50, regParam = 0)\n",
    "\n",
    "#pipeline = Pipeline(stages=[gbt])\n",
    "pipeline = Pipeline(stages=[lgr])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(rest)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(validation)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Churn\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.05947955390334572\n",
      "recall =  1.0\n",
      "f1_score =  0.11228070175438597\n",
      "accuracy =  0.8473495187473065\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
