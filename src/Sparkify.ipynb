{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkML-Customer Churn Prediction with sparkify dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, col, concat, desc, explode, lit, min, max, split, udf\n",
    "from pyspark.sql.types import IntegerType, DoubleType, BooleanType\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, GBTClassifier, LinearSVC\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import Imputer, CountVectorizer, IDF, Normalizer, PCA, RegexTokenizer, StandardScaler, StopWordsRemover, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist='Martin Orford', auth='Logged In', firstName='Joseph', gender='M', itemInSession=20, lastName='Morales', length=597.55057, level='free', location='Corpus Christi, TX', method='PUT', page='NextSong', registration=1532063507000, sessionId=292, song='Grand Designs', status=200, ts=1538352011000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36\"', userId='293'),\n",
       " Row(artist=\"John Brown's Body\", auth='Logged In', firstName='Sawyer', gender='M', itemInSession=74, lastName='Larson', length=380.21179, level='free', location='Houston-The Woodlands-Sugar Land, TX', method='PUT', page='NextSong', registration=1538069638000, sessionId=97, song='Bulls', status=200, ts=1538352025000, userAgent='\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.143 Safari/537.36\"', userId='98')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': 'iam-ServiceId-dcd80924-823c-4e98-a4a1-9008ddc1a9a8',\n",
    "    'iam_service_endpoint': 'https://iam.bluemix.net/oidc/token',\n",
    "    'api_key': 'kV730WOitMi206CFroom3MFI13vBRa2kD8MNECWTx1tQ'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_522f6938d423453eb1317cb06aa5d727_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Since JSON data can be semi-structured and contain additional metadata, it is possible that you might face issues with the DataFrame layout.\n",
    "# Please read the documentation of 'SparkSession.read()' to learn more about the possibilities to adjust the data loading.\n",
    "# PySpark documentation: http://spark.apache.org/docs/2.0.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.json\n",
    "\n",
    "df = spark.read.json(cos.url('medium-sparkify-dataset.json', 'sparkify-donotdelete-pr-1hvb6rdsl1zaqx'))\n",
    "df.take(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rows with len(userid) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(\"length(userid) != 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|          location|method|    page| registration|sessionId|         song|status|           ts|           userAgent|userId|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "|Martin Orford|Logged In|   Joseph|     M|           20| Morales|597.55057| free|Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid = df.dropna(how = 'any', subset = ['userId', 'sessionId'])\n",
    "df_valid.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill empty entries in  columns:\n",
    "StringIndexer in pyspark cannot deal with empty entries. The empty entries in columns with string type should be filled with \"None\" to avoid exception. The numeric columns are filled with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = ['artist', 'gender', 'location', 'song', 'userAgent']\n",
    "num_columns = ['itemInSession', 'length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_string_null(df, columns):\n",
    "    '''\n",
    "        input: \n",
    "                df : spark data frame.\n",
    "                columns : columns in df that are of string type.\n",
    "        \n",
    "        return: transformed data frame.\n",
    "    '''\n",
    "    for column in columns:\n",
    "        df = df.withColumn(column, F.when(F.isnull(F.col(column)),\"NONE\").otherwise(F.col(column)))\n",
    "    return df\n",
    "\n",
    "def fill_num_null(df, columns):\n",
    "    #transform to double type\n",
    "    for column in columns:\n",
    "        df = df.withColumn(column, df[column].cast(DoubleType()))\n",
    "        \n",
    "    imputer = Imputer(inputCols = columns, outputCols = columns)\n",
    "    df = imputer.fit(df).transform(df)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = fill_string_null(df_valid, string_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = fill_num_null(df_valid, num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "### Define Churn\n",
    "\n",
    "Create a column `Churn` to use as the label for your model. `Cancellation Confirmation`,`Cancel`,`Submit Downgrade` and `Downgrade` events are considered to define churn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explory visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+------+-------------+--------+------------------+-----+-------------+------+---------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|artist|     auth|firstName|gender|itemInSession|lastName|            length|level|     location|method|     page| registration|sessionId|song|status|           ts|           userAgent|userId|\n",
      "+------+---------+---------+------+-------------+--------+------------------+-----+-------------+------+---------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "|  NONE|Logged In|    Sofia|     F|        268.0|  Gordon|248.66459278007738| paid|Rochester, MN|   GET|Downgrade|1533175710000|      162|NONE|   200|1538352336000|\"Mozilla/5.0 (Mac...|   163|\n",
      "|  NONE|Logged In|    Sofia|     F|        296.0|  Gordon|248.66459278007738| paid|Rochester, MN|   GET|Downgrade|1533175710000|      162|NONE|   200|1538358258000|\"Mozilla/5.0 (Mac...|   163|\n",
      "+------+---------+---------+------+-------------+--------+------------------+-----+-------------+------+---------+-------------+---------+----+------+-------------+--------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.filter('page == \"Cancellation Confirmation\" or page == \"Downgrade\" or page == \"Cancel\" or page == \"Submit Downgrade\"').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                page|\n",
      "+--------------------+\n",
      "|              Cancel|\n",
      "|    Submit Downgrade|\n",
      "|         Thumbs Down|\n",
      "|                Home|\n",
      "|           Downgrade|\n",
      "|         Roll Advert|\n",
      "|              Logout|\n",
      "|       Save Settings|\n",
      "|Cancellation Conf...|\n",
      "|               About|\n",
      "|            Settings|\n",
      "|     Add to Playlist|\n",
      "|          Add Friend|\n",
      "|            NextSong|\n",
      "|           Thumbs Up|\n",
      "|                Help|\n",
      "|             Upgrade|\n",
      "|               Error|\n",
      "|      Submit Upgrade|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.select('page').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Churn for each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_class = udf(lambda x : 1 if x in [\"Cancellation Confirmation\", \"Downgrade\", 'Submit Downgrade', 'Cancel'] else 0, IntegerType())\n",
    "df_valid = df_valid.withColumn('Churn', label_class(df_valid.page))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|Churn| count|\n",
      "+-----+------+\n",
      "|    1|  4126|\n",
      "|    0|523879|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.groupby(df_valid.Churn).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the following fields as features to train our model:\n",
    "\n",
    "* string_columns = ['artist', 'gender', 'location', 'song', 'userAgent']\n",
    "\n",
    "* num_columns = ['itemInSession', 'length']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode gender feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodeFeatures(df, columns):\n",
    "    '''\n",
    "        encode categorical features to numerical features.\n",
    "        input:\n",
    "            df: spark dataframe.\n",
    "            columns: categorical feature columns.\n",
    "    '''\n",
    "    for column in columns:\n",
    "        indexer = StringIndexer(inputCol = column, outputCol = column + \"_encode\")\n",
    "        df = indexer.fit(df).transform(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+\n",
      "|       artist|     auth|firstName|gender|itemInSession|lastName|   length|level|          location|method|    page| registration|sessionId|         song|status|           ts|           userAgent|userId|Churn|artist_encode|gender_encode|location_encode|song_encode|userAgent_encode|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+\n",
      "|Martin Orford|Logged In|   Joseph|     M|         20.0| Morales|597.55057| free|Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|    0|       3931.0|          0.0|           26.0|     4600.0|             0.0|\n",
      "+-------------+---------+---------+------+-------------+--------+---------+-----+------------------+------+--------+-------------+---------+-------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid = encodeFeatures(df_valid, string_columns)\n",
    "df_valid.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate feature column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* string_columns = ['artist', 'gender', 'location', 'song', 'userAgent']\n",
    "* num_columns = ['itemInSession', 'length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df_valid.drop('features')\n",
    "df_valid = df_valid.drop('rawFeatures')\n",
    "assembler = VectorAssembler(inputCols = [\"artist_encode\",\"gender_encode\",\\\n",
    "                                       \"location_encode\",\"song_encode\",\\\n",
    "                                       \"userAgent_encode\", \"itemInSession\",\\\n",
    "                                       \"length\"], outputCol = \"rawFeatures\")\n",
    "df_valid = assembler.transform(df_valid)\n",
    "# normalize features\n",
    "scaler = Normalizer(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "df_valid = scaler.transform(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+--------------------+--------------------+\n",
      "|           artist|     auth|firstName|gender|itemInSession|lastName|   length|level|            location|method|    page| registration|sessionId|              song|status|           ts|           userAgent|userId|Churn|artist_encode|gender_encode|location_encode|song_encode|userAgent_encode|         rawFeatures|            features|\n",
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+--------------------+--------------------+\n",
      "|    Martin Orford|Logged In|   Joseph|     M|         20.0| Morales|597.55057| free|  Corpus Christi, TX|   PUT|NextSong|1532063507000|      292|     Grand Designs|   200|1538352011000|\"Mozilla/5.0 (Mac...|   293|    0|       3931.0|          0.0|           26.0|     4600.0|             0.0|[3931.0,0.0,26.0,...|[0.64650667128954...|\n",
      "|John Brown's Body|Logged In|   Sawyer|     M|         74.0|  Larson|380.21179| free|Houston-The Woodl...|   PUT|NextSong|1538069638000|       97|             Bulls|   200|1538352025000|\"Mozilla/5.0 (Mac...|    98|    0|       2577.0|          0.0|            9.0|    12367.0|             5.0|[2577.0,0.0,9.0,1...|[0.20389945259065...|\n",
      "|          Afroman|Logged In| Maverick|     M|        184.0|Santiago|202.37016| paid|Orlando-Kissimmee...|   PUT|NextSong|1535953455000|      178|Because I Got High|   200|1538352118000|\"Mozilla/5.0 (Mac...|   179|    0|       1320.0|          0.0|           27.0|     3436.0|             5.0|[1320.0,0.0,27.0,...|[0.35761906426868...|\n",
      "+-----------------+---------+---------+------+-------------+--------+---------+-----+--------------------+------+--------+-------------+---------+------------------+------+-------------+--------------------+------+-----+-------------+-------------+---------------+-----------+----------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_valid.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "Split the full dataset into train, test sets. Evaluate the metrics of the various models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only paid users are of interest.\n",
    "rest, validation = df_valid.filter(df_valid.level == 'paid').randomSplit([0.75, 0.25], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add weight column to solve imbalanced dataset problem:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use the information of training dataset to calculate the balanced class weight. \n",
    "# Do not use information from test dataset to avoid fake score.\n",
    "def addWeight(df_train, df_test, gammas = [1]):\n",
    "    '''\n",
    "        add weight column to solve imbalanced problem.\n",
    "        The weightCol field is used in logistic regression model.\n",
    "        input:\n",
    "            df_train : training data frame.\n",
    "            df_test : test data frame.\n",
    "            \n",
    "        return:\n",
    "            training and test dataframe after transformation. \n",
    "    '''\n",
    "    total_instances = df_train.count()\n",
    "    negative_instances = df_train.filter(df_train.Churn == 0).count()\n",
    "    for gamma in gammas:\n",
    "        balance_ratio = (total_instances - negative_instances * gamma) / total_instances\n",
    "        print(\"balance_ratio = \", balance_ratio)\n",
    "        print(\"balance_ratio = \", 1 - balance_ratio)\n",
    "        add_weightCol = udf(lambda x : balance_ratio if x == 0 else (1 - balance_ratio), DoubleType())\n",
    "        df_train = df_train.withColumn('weightCol%d'%(gamma*1000), add_weightCol(df_train.Churn))\n",
    "        df_test = df_test.withColumn('weightCol%d'%(gamma*1000), add_weightCol(df_test.Churn))\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balance_ratio =  0.009737619926728714\n",
      "balance_ratio =  0.9902623800732713\n"
     ]
    }
   ],
   "source": [
    "rest, validation = addWeight(rest, validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Churn|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[0.33881679736696...|\n",
      "|       0.0|    0|[0.33896164916210...|\n",
      "|       0.0|    0|[0.16668946441272...|\n",
      "|       0.0|    0|[0.55517912002349...|\n",
      "|       0.0|    0|[0.49187204269535...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a LogisticRegression model.\n",
    "lgr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                         weightCol = 'weightCol1000', maxIter = 300, regParam = 0)\n",
    "\n",
    "#pipeline = Pipeline(stages=[gbt])\n",
    "pipeline = Pipeline(stages=[lgr])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(rest)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(validation)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Churn\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(predictions.filter(predictions.prediction == predictions.Churn).count())\n",
    "# print(predictions.count())\n",
    "# print(predictions.filter(predictions.Churn == 1).count())\n",
    "# print(predictions.filter(predictions.prediction == 1).count())\n",
    "# print(predictions.filter(predictions.prediction == 0).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric Evalulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(predictions):\n",
    "    tp = predictions.filter(predictions.prediction == predictions.Churn).\\\n",
    "                 filter(predictions.prediction == 1).count()\n",
    "    tn = predictions.filter(predictions.prediction == predictions.Churn).\\\n",
    "                     filter(predictions.prediction == 0).count()\n",
    "    fp = predictions.filter(predictions.prediction != predictions.Churn).\\\n",
    "                     filter(predictions.prediction == 1).count()\n",
    "    fn = predictions.filter(predictions.prediction != predictions.Churn).\\\n",
    "                     filter(predictions.prediction == 0).count()\n",
    "    print(\"true positive\", tp)\n",
    "    print(\"true negative\", tn)\n",
    "    print(\"false positive\", fp)\n",
    "    print(\"false negative\", tn)\n",
    "    print(\"precision = \",precision(tp, fp))\n",
    "    print(\"recall = \",recall(tp, fn))\n",
    "    print(\"f1_score = \",f1_score(tp, tn, fp, fn))\n",
    "    print(\"accuracy = \", accuracy(tp, tn, fp, fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    #add one smoothness\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    #add one smoothness\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def f1_score(tp, tn, fp, fn):\n",
    "    pre = precision(tp, fp)\n",
    "    rec = recall(tp, fn)\n",
    "    return 2 * pre * rec / (pre + rec)\n",
    "\n",
    "def accuracy(tp, tn, fp, fn):\n",
    "    return (tp + tn) / (tp + tn + fp + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true positive 0\n",
      "true negative 103407\n",
      "false positive 0\n",
      "false negative 103407\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-230-a535027cfbf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-226-be629c1269ed>\u001b[0m in \u001b[0;36mprint_metrics\u001b[0;34m(predictions)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"false positive\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"false negative\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"precision = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"f1_score = \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-229-997e5661817b>\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(tp, fp)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m#add one smoothness\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear support vector machine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a87d4589eb5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Train model. This also runs the indexers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Make predictions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/spark/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \"\"\"\n\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1156\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1158\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1159\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1160\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    906\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/ibm/conda/miniconda3/lib/python3.5/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                        weightCol = 'weightCol1000', maxIter = 50, regParam = 0)\n",
    "\n",
    "# Chain indexers and GBT in a Pipeline\n",
    "#pipeline = Pipeline(stages=[gbt])\n",
    "pipeline = Pipeline(stages=[svc])\n",
    "\n",
    "# Train model. This also runs the indexers.\n",
    "model = pipeline.fit(rest)\n",
    "\n",
    "# Make predictions.\n",
    "predictions_svm = model.transform(validation)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions_svm.select(\"prediction\", \"Churn\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = predictions_svm.filter(predictions_svm.prediction == predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 1).count()\n",
    "tn = predictions_svm.filter(predictions_svm.prediction == predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 0).count()\n",
    "fp = predictions_svm.filter(predictions_svm.prediction != predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 1).count()\n",
    "fn = predictions_svm.filter(predictions_svm.prediction != predictions_svm.Churn).\\\n",
    "                 filter(predictions_svm.prediction == 0).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positive Samples :  1008\n",
      "True Negative Samples :  87468\n",
      "False Positive Samples :  15939\n",
      "False Negative Samples :  0\n"
     ]
    }
   ],
   "source": [
    "print(\"True Positive Samples : \", tp)\n",
    "print(\"True Negative Samples : \", tn)\n",
    "print(\"False Positive Samples : \", fp)\n",
    "print(\"False Negative Samples : \", fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_metrics(predictions_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tunning models using k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                        threshold = 0.6, maxIter = 5, regParam = 0)\n",
    "\n",
    "pipeline = Pipeline(stages=[lgr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lgr.weightCol, ['weightCol970','weightCol990','weightCol995',\\\n",
    "                             'weightCol1000','weightCol1004'])\\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(rawPredictionCol = 'prediction', \\\n",
    "                                                                  labelCol = 'Churn', metricName = 'areaUnderPR'),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_q1 = crossval.fit(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = cvModel_q1.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010321893090731194,\n",
       " 0.015602875059288215,\n",
       " 0.026231108951647014,\n",
       " 0.026298931421153207,\n",
       " 0.02122385614697736]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel_q1.avgMetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation conclusin:\n",
    "\n",
    "This suggests that gamma = 1 gives us best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use best hyper-parameter to train model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------+\n",
      "|prediction|Churn|            features|\n",
      "+----------+-----+--------------------+\n",
      "|       0.0|    0|[2916.0,0.0,7.0,8...|\n",
      "|       0.0|    0|[2916.0,0.0,57.0,...|\n",
      "|       0.0|    0|[2916.0,0.0,25.0,...|\n",
      "|       0.0|    0|[6571.0,1.0,48.0,...|\n",
      "|       0.0|    0|[7109.0,1.0,101.0...|\n",
      "+----------+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a LogisticRegression model.\n",
    "lgr = LogisticRegression(labelCol=\"Churn\", featuresCol=\"features\", \\\n",
    "                        weightCol = 'weightCol', threshold = 0.6, maxIter = 200, regParam = 0)\n",
    "\n",
    "#pipeline = Pipeline(stages=[gbt])\n",
    "pipeline = Pipeline(stages=[lgr])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(rest)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(validation)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"Churn\", \"features\").show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision =  0.05947955390334572\n",
      "recall =  1.0\n",
      "f1_score =  0.11228070175438597\n",
      "accuracy =  0.8473495187473065\n"
     ]
    }
   ],
   "source": [
    "print_metrics(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
